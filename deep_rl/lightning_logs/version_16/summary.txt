python a2c_train.py --batch_size=128 --lr=5e-5 --env=WordleEnvFull-v0 --n_hidden=2 --hidden_size=512 --prob_cheat=0.05 --max_steps=500000
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice:              3
wandb: You chose 'Don't visualize my results'
wandb: Tracking run with wandb version 0.12.14
wandb: W&B syncing is set to `offline` in this directory.
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Global seed set to 123
GPU available: True, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
C:\Users\shreyasb\Anaconda3\envs\eecs545\lib\site-packages\pytorch_lightning\trainer\trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  rank_zero_warn(

  | Name | Type     | Params
----------------------------------
0 | net  | SumChars | 763 K
----------------------------------
763 K     Trainable params
0         Non-trainable params
763 K     Total params
3.055     Total estimated model params size (MB)
C:\Users\shreyasb\Anaconda3\envs\eecs545\lib\site-packages\pytorch_lightning\trainer\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 0: : 0it [00:00, ?it/s]C:\Users\shreyasb\Anaconda3\envs\eecs545\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\result.py:227: UserWarning: You called `self.log('total wins:', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
C:\Users\shreyasb\Anaconda3\envs\eecs545\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\result.py:227: UserWarning: You called `self.log('total losses:', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
Epoch 0: : 500001it [14:46:58,  9.40it/s, loss=0.253, v_num=16, train_loss:=0.491, loss_ratio:=0.213, total wins:=9.17e+6, total losses:=4.22e+6]     

wandb: Waiting for W&B process to finish... (success).
wandb:
wandb: 
wandb: Run history:
wandb:  Unique Words used ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  avg_winning_turns ▁▁▂▃▄▂▃▄▄▃▂▃▄▅▄▅▇▄▄▅▅▄▄▃▆▅▇▆▆▆▆▄▆▆▆▇█▇█▅
wandb:        global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:         lose_ratio ██▇▇▇▇▆▇▆▆▆▆▆▅▅▅▅▄▅▄▄▄▄▄▃▃▃▃▂▃▃▂▁▂▁▂▁▁▁▁
wandb:             losses ██▇▇▇▇▆▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▄▃▃▂▃▃▂▁▂▁▂▁▂▁▂
wandb:    reward_per_game ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█▇█▇
wandb:     reward_per_win ▁▁▂▃▃▁▃▃▃▃▂▃▃▄▄▄▅▄▃▄▄▅▄▃▅▅▆▆▆▆▆▅▆▅▆▆█▆▇▆
wandb: total_games_played ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:       total_losses ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████
wandb:         total_wins ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███
wandb:         train_loss ▇▄▆▅▄▅▂█▆▄▅▄▅▇▇▄▅▄▅▆▅▅▅▆▄▄▄▅▅▅▅▅▆▄▅▄▇▆▅▁
wandb:               wins ▁▁▂▂▁▂▃▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▆▆▇█▇█▇████
wandb:
wandb: Run summary:
wandb:  Unique Words used 1
wandb:  avg_winning_turns 4.27078
wandb:        global_step 499800
wandb:         lose_ratio 0.21294
wandb:             losses 1175
wandb:    reward_per_game 32.5932
wandb:     reward_per_win 48.60698
wandb: total_games_played 13396589
wandb:       total_losses 4223103
wandb:         total_wins 9173486
wandb:         train_loss 0.49068
wandb:               wins 4343
wandb:
wandb: You can sync this run to the cloud by running:
wandb: wandb sync C:\Users\shreyasb\Desktop\EECS545\4-letter\Deep-RL-Wordle-\deep_rl\wandb\offline-run-20220417_144215-lcsnc3gk
wandb: Find logs at: .\wandb\offline-run-20220417_144215-lcsnc3gk\logs

WordleEnvFull-v0 (4-Letter Dataset)
Character level rl 
prob_cheat=0.05
wins on 1870/2348 Words
n_hidden = 2
hidden_size=512
batch_size=128
lr=5e-5
max_steps=500k + 500k
