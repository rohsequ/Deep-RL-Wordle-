python a2c_train.py --env=WordleEnvFull-v0 --batch_size=128 --lr=5e-5 --n_hidden=2 --hidden_size=512 --prob_cheat=0.05 --max_steps=500000
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose 'Don't visualize my results'
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Global seed set to 123
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs

  | Name | Type     | Params
----------------------------------
0 | net  | SumChars | 763 K 
----------------------------------
763 K     Trainable params
0         Non-trainable params
763 K     Total params
3.055     Total estimated model params size (MB)
/home/shreyas/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 0: : 0it [00:00, ?it/s]/home/shreyas/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:226: UserWarning: You called `self.log('total wins:', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/shreyas/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:226: UserWarning: You called `self.log('total losses:', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
Epoch 0: : 500001it [8:10:39, 16.98it/s, loss=0.307, v_num=15, train_loss:=-.144, loss_ratio:=0.429, total wins:=5.12e+6, total losses:=7.35e+6]   

wandb: Waiting for W&B process to finish, PID 4897... (success).
wandb: Run history:
wandb:    Unique Words used ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    avg_winning_turns ▂▁▁▂▂▃▂▃▄▃▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇█▇█████
wandb:          global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:           lose_ratio █████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:               losses █████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:      reward_per_game ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████
wandb:       reward_per_win ▁▁▁▂▃▃▃▃▃▃▄▄▅▄▅▅▅▅▅▅▆▆▆▆▆▇▆▇▇▆▇█▇█▇█████
wandb:   total_games_played ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:         total_losses ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████
wandb:           total_wins ▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇██
wandb:           train_loss ▄▄▆▅▄▅▆▆▇█▃▃▃▆▅▆▄▄▃▆▆▇█▄▅▅▆▆▄▆▃▅▂▄▃▄▁▃▃▄
wandb:                 wins ▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:    Unique Words used 1
wandb:    avg_winning_turns 4.10266
wandb:          global_step 499800
wandb:           lose_ratio 0.42931
wandb:               losses 2235
wandb:      reward_per_game 14.58035
wandb:       reward_per_win 46.26759
wandb:   total_games_played 12468582
wandb:         total_losses 7348339
wandb:           total_wins 5120243
wandb:           train_loss -0.14398
wandb:                 wins 2971
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/shreyas/EECS545/current-code/4letter/Deep-RL-Wordle-/deep_rl/wandb/offline-run-20220417_030950-33jult3a
wandb: Find logs at: ./wandb/offline-run-20220417_030950-33jult3a/logs/debug.log
wandb:

WordleEnvFull-v0 (4-Letter Dataset)
Character level rl 
prob_cheat=0.05
wins on 1266/2348 Words
n_hidden = 2
hidden_size=512
batch_size=128
lr=5e-5
max_steps=500k

