C:\Users\hardi\anaconda3\lib\site-packages\pytorch_lightning\trainer\trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  rank_zero_warn(
C:\Users\hardi\anaconda3\lib\site-packages\pytorch_lightning\trainer\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\module.py(248)loss()
-> logprobs, values = self.net(states)
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\module.py(251)loss()
-> with torch.no_grad():
tensor([[-2.3375, -2.3642, -2.1918, -2.1711, -2.2681, -2.6368, -2.3572, -2.6286,
         -2.2171, -2.0193]], grad_fn=<ClampBackward1>)
tensor([[-0.0256]], grad_fn=<AddmmBackward0>)
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\module.py(253)loss()
-> advs = returns - values * returns.std() + returns.mean()
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\module.py(255)loss()
-> advs = (advs - advs.mean()) / (advs.std() + self.eps)
tensor([[nan]])
tensor([2.2575])
tensor([[-0.0256]], grad_fn=<AddmmBackward0>)
tensor(nan)
--KeyboardInterrupt--

