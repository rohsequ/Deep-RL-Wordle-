C:\Users\hardi\anaconda3\lib\site-packages\pytorch_lightning\trainer\trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  rank_zero_warn(
C:\Users\hardi\anaconda3\lib\site-packages\pytorch_lightning\trainer\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(44)forward()
-> input = x.unsqueeze_(1) # L, N, Hin
<built-in function input>
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(45)forward()
-> h0 = torch.randn(n_hidden, 1, hidden_size) # D*numlayers, Hout D = 1 for uni-directional
*** AttributeError: 'Tensor' object has no attribute 'shaPE'
torch.Size([1, 1, 157])
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(46)forward()
-> c0 = torch.randn(n_hidden, 1, hidden_size) # D*numlayers, Hout
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(47)forward()
-> out_lstm, (hn, cn) = self.f1(input.float(), (h0, c0))
--Call--
> c:\users\hardi\anaconda3\lib\site-packages\torch\nn\modules\module.py(1164)__getattr__()
-> def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
> c:\users\hardi\anaconda3\lib\site-packages\torch\nn\modules\module.py(1165)__getattr__()
-> if '_parameters' in self.__dict__:
> c:\users\hardi\anaconda3\lib\site-packages\torch\nn\modules\module.py(1166)__getattr__()
-> _parameters = self.__dict__['_parameters']
