C:\Users\hardi\anaconda3\lib\site-packages\pytorch_lightning\trainer\trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  rank_zero_warn(
C:\Users\hardi\anaconda3\lib\site-packages\pytorch_lightning\trainer\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
> d:\eecs 545 project\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(51)forward()
-> actor_a = torch.clamp(torch.log_softmax(
