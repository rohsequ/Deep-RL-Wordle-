C:\Users\Hardik\anaconda3\lib\site-packages\pytorch_lightning\trainer\trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  rank_zero_warn(
C:\Users\Hardik\anaconda3\lib\site-packages\pytorch_lightning\trainer\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]> c:\users\hardik\downloads\wordle_a2c_lstm_new-hardik\deep_rl\a2c\sumchars.py(53)forward()
-> y = self.f0(x.float())
torch.Size([1, 157])
Sequential(
  (0): Linear(in_features=157, out_features=128, bias=True)
  (1): ReLU()
  (2): LSTM(128, 128)
  (3): Linear(in_features=128, out_features=130, bias=True)
  (4): ReLU()
)
